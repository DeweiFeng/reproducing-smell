# reproducing-smell ğŸ‘ƒğŸˆ

1. Sensor read â†’ raw vector S
The dataset loads each gas-nose CSV and returns a tensor `sensor` of shape `[seq_len, features]`.

2. SmellNet (LSTM) â†’ embeddings zâ‚â€¦zâ‚…â‚€
In `MultimodalOdorNet`, `self.smell_enc` runs the sensor sequence through an LSTM and projects its output into a fixed embedding for each of SmellNet V0.1 50 odor channels (the záµ¢).

3. Î±-weights & sum â†’ unified â€œodorâ€ embedding
A learned linear layer (`smell_proj`) applies implicit Î±áµ¢ weights to those záµ¢ and sums them into one fused smell vector.

4. Multimodal fusion â†’ mixture ratios Î²â‚â€¦Î²â‚â‚‚
In parallel, Qwen encodes our image+text; all three modality embeddings (vision, text, smell) are projected into the same space, stacked, and passed through a TransformerEncoder. Finally, `output_head` Softmaxes to produce Î²â‚â€¦Î²â‚â‚‚â€”the mix proportions over 12 base scents.

7. playSmell(id,duration)
The script picks the Î²â±¼ with highest score (â†’ `scent_id`), calls `nw.playSmell(scent_id, duration)`, and sends that hex command over serial to the NeckWear device to diffuse the predicted fragrance.

So this code exactly implements:

- Stage 1 (Î±) = LSTM + `smell_proj`
- Stage 2 (Î²) = Transformer fusion + `output_head`
- Runtime = loop over data â†’ select top Î² â†’ `playSmell`

With a properly trained `checkpoint.pt`, this runs end-to-end as drawn.
